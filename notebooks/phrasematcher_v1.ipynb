{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "phrasematcher_v1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata": {
      "interpreter": {
        "hash": "d91c48f48189c56664de038e34a1f930f7a4a539cbe8f04b9f1e81ab0844a6c3"
      }
    },
    "orig_nbformat": 2
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v8tIGSJKNHP"
      },
      "source": [
        "# SETUP ENVIRONMENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p93Pb0mIC8cv",
        "outputId": "3cb46190-9202-4035-e034-58222f34c5e8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cloning into 'clinical_NLP_SE'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Et63n-mVcR",
        "outputId": "37d3b331-9761-4d46-d6cb-97e1f4ad7cdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Access token from drive without exposing in text (accsessible in logfiles lol. Better than nothing)\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "token = open(\"/content/drive/MyDrive/token/colab.txt\", \"r\").read()\n",
        "tkn = token.split(\"=\")\n",
        "\n",
        "repo_name = \"callebalik/clinical_NLP_SE.git\"\n",
        "cmd_string = 'git clone https://{0}@github.com/{1}'.format(tkn[1], repo_name)\n",
        "\n",
        "print(cmd_string)\n",
        "!{cmd_string}\n",
        "cmd_string = \"\" # removing the variable\n",
        "print(cmd_string)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "git clone https://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@github.com/callebalik/clinical_NLP_SE.git\n",
            "Cloning into 'clinical_NLP_SE'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 45 (delta 8), reused 38 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3gnBRnNOsg6"
      },
      "source": [
        "# Set paths for the environment \n",
        "import os \n",
        "from pathlib import Path\n",
        "\n",
        "drive_path = Path('/content/drive/MyDrive/bioNLP/')\n",
        "code_path = Path('/content/clinical_NLP_SE/')\n",
        "models_path = Path('/content/drive/My Drive/bioNLP/models/')"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We3MQcmkDsm4"
      },
      "source": [
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1p1i1iHKd3R"
      },
      "source": [
        "# GET DATA AND MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsf5vHeSJ5Tg"
      },
      "source": [
        "## Gettings the Swedish models \n",
        "\n",
        "The models can be downloaded with the commands below. For colab this is slow and they are instead mounted from a google drive where they have been downloaded\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKfI3y19GVvZ"
      },
      "source": [
        "# Get models from Kungbib https://github.com/Kungbib/swedish-spacy\n",
        "#!wget https://data.kb.se/datasets/2020/10/swedish_nlp/spacy/sv_tagger-0.0.0.tar.gz\n",
        "\n",
        "# Older models\n",
        "#!wget https://data.kb.se/datasets/2020/10/swedish_nlp/spacy/sv_model_upos.zip\n",
        "#!wget https://data.kb.se/datasets/2020/10/swedish_nlp/spacy/sv_model_xpos.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ALJypSdZJ3d8",
        "outputId": "1ce7ee02-2d5c-4d6c-b4c3-bd3af5825f2e"
      },
      "source": [
        "import spacy\n",
        "!pip install \"/content/drive/My Drive/bioNLP/models/sv_pipeline-0.0.0.tar.gz\""
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./drive/My Drive/bioNLP/models/sv_tagger-0.0.0 (1).tar.gz\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from sv-tagger==0.0.0) (3.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (56.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (0.3.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (0.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (1.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (0.5.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (4.41.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (8.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (1.7.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (3.4.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2020.12.5)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.1->sv-tagger==0.0.0) (2.4.7)\n",
            "Building wheels for collected packages: sv-tagger\n",
            "  Building wheel for sv-tagger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sv-tagger: filename=sv_tagger-0.0.0-cp37-none-any.whl size=464344746 sha256=f82732dc4cd934cca93e3021f3aad0b3679371f21427c2739399c4d4986cde70\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/36/63/872f9e11d2a0bbecbd9f80841c3658d197e53775600fe8fb2d\n",
            "Successfully built sv-tagger\n",
            "Installing collected packages: sv-tagger\n",
            "  Found existing installation: sv-tagger 0.0.0\n",
            "    Uninstalling sv-tagger-0.0.0:\n",
            "      Successfully uninstalled sv-tagger-0.0.0\n",
            "Successfully installed sv-tagger-0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWPMEmhlrWDs"
      },
      "source": [
        "### Note from Kungbib\n",
        "\n",
        "** UPDATE February 2021 **: We are adding two transformer-based models trained in spaCy 3.0. They are available to download at the same link given below.\n",
        "One model is a complete pipeline with UPOS tagger, parser, sentencer, ner and lemmatizer (sv_pipeline-0.0.0.tar.gz). Unfortunately the lemmatizer is not yet trainable in spaCy, so the performance is as good as the quality of the rules/lookup tables available for Swedish (i.e. not very good). \n",
        "\n",
        "If you need a Swedish lemmatizer we advise you for the moment to have a look at Stanza, efselab or lemmy.ww\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The other model is a XPOS tagger in case you need language-specific part-of-speech tags (sv_tagger-0.0.0.tar.gz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfzk7npYsSMQ"
      },
      "source": [
        "# Analyzing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie3y0lSVsXoY",
        "outputId": "a61de3c4-e3f0-4544-c8c8-8db84a758dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "nlp = spacy.load(\"sv_pipeline\")\n",
        "doc = nlp(\"Jag gillar London och Berlin.\")\n",
        "width = 15\n",
        "for token in doc:\n",
        "    print(f\"{token.text: <{width}} {token.tag_: <{width}} {token.dep_: <{width}}\")\n",
        "    \n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-160-c26bc2323983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sv_tagger\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Jag gillar London och Berlin.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{token.text: <{width}} {token.tag_: <{width}} {token.dep_: <{width}}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m def load(\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSimpleFrozenDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \"\"\"Simplified implementation of a frozen dict, mainly used as default\n\u001b[0m\u001b[1;32m    160\u001b[0m     function or method argument (for arguments that should default to empty\n\u001b[1;32m    161\u001b[0m     dictionary). Will raise an error if user or spaCy attempts to add to dict.\n",
            "\u001b[0;31mOSError\u001b[0m: [E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary."
          ]
        }
      ]
    }
  ]
}