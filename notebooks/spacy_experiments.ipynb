{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"entity_ruler\")\r\n",
    "def custom_sentencizer(doc):\r\n",
    "    for i, token in enumerate(doc[:-2]):\r\n",
    "        # Define sentence start if pipe + titlecase token\r\n",
    "        if token.text == \"|\" and doc[i + 1].is_title:\r\n",
    "            doc[i + 1].is_sent_start = True\r\n",
    "        else:\r\n",
    "            # Explicitly set sentence start to False otherwise, to tell\r\n",
    "            # the parser to leave those tokens alone\r\n",
    "            doc[i + 1].is_sent_start = False\r\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def to_disk(\r\n",
    "        self, path: Union[str, Path], *, exclude: Iterable[str] = SimpleFrozenList()\r\n",
    "    ) -> None:\r\n",
    "        \"\"\"Save the entity ruler patterns to a directory. The patterns will be\r\n",
    "        saved as newline-delimited JSON (JSONL).\r\n",
    "        path (str / Path): The JSONL file to save.\r\n",
    "        DOCS: https://spacy.io/api/entityruler#to_disk\r\n",
    "        \"\"\"\r\n",
    "        path = ensure_path(path)\r\n",
    "        cfg = {\r\n",
    "            \"overwrite\": self.overwrite,\r\n",
    "            \"phrase_matcher_attr\": self.phrase_matcher_attr,\r\n",
    "            \"ent_id_sep\": self.ent_id_sep,\r\n",
    "        }\r\n",
    "        serializers = {\r\n",
    "            \"patterns\": lambda p: srsly.write_jsonl(\r\n",
    "                p.with_suffix(\".jsonl\"), self.patterns\r\n",
    "            ),\r\n",
    "            \"cfg\": lambda p: srsly.write_json(p, cfg),\r\n",
    "        }\r\n",
    "        if path.suffix == \".jsonl\":  # user wants to save only JSONL\r\n",
    "            srsly.write_jsonl(path, self.patterns)\r\n",
    "        else:\r\n",
    "            to_disk(path, serializers, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "id": "kI5OxexkQWqs",
    "outputId": "be3204c4-3bbb-4111-a74b-bcf227fd239e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "Laura flew to Silicon Valley.\n",
       "1.0\n",
       "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0}\n",
       "{'PRS': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'LOC': {'p': 1.0, 'r': 1.0, 'f': 1.0}}\n"
      ]
     },
     "output_type": "unknown"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Laura\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRS</span>\n",
       "</mark>\n",
       " flew to \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Silicon Valley\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "output_type": "unknown"
    },
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "{'doc_annotation': {'cats': {}, 'entities': ['U-PRS', 'O', 'O', 'B-LOC', 'L-LOC', 'O'], 'links': {}}, 'token_annotation': {'ORTH': ['Laura', 'flew', 'to', 'Silicon', 'Valley', '.'], 'SPACY': [True, True, True, True, False, False], 'TAG': ['', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', ''], 'POS': ['', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5], 'DEP': ['', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0]}}\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "from spacy.scorer import Scorer\r\n",
    "from spacy.tokens import Doc\r\n",
    "from spacy.training import Example\r\n",
    "\r\n",
    "# Training data for an entity recognizer (option 2)\r\n",
    "doc = nlp(\"Laura flew to Silicon Valley.\")\r\n",
    "gold_dict = {\"entities\": [(0, 5, \"PRS\"), (14, 28, \"LOC\")]}\r\n",
    "example = Example.from_dict(doc, gold_dict)\r\n",
    "\r\n",
    "print(example.text)\r\n",
    "examples = []\r\n",
    "examples.append(example)\r\n",
    "\r\n",
    "# Spacy V3 The Language.evaluate method now takes a batch of Example objects instead of tuples of Doc and GoldParse objects.\r\n",
    "scores = nlp.evaluate(examples)\r\n",
    "print(scores[\"ents_p\"])\r\n",
    "\r\n",
    "\r\n",
    "# Default scoring pipeline\r\n",
    "#scorer = Scorer()\r\n",
    "\r\n",
    "# Provided scoring pipeline\r\n",
    "scores = Scorer.score_tokenization(examples)\r\n",
    "print(scores)\r\n",
    "\r\n",
    "\r\n",
    "# Returns A dictionary containing the PRF scores under the keys {attr}_p, {attr}_r, {attr}_f and the per-type PRF scores under {attr}_per_type\r\n",
    "spans = Scorer.score_spans(examples, \"ents\")\r\n",
    "print(spans[\"ents_per_type\"])\r\n",
    "\r\n",
    "from spacy import displacy\r\n",
    "\r\n",
    "colors = {\r\n",
    "          \"SYM\": \"linear-gradient(90deg, #99154e, #99154e)\",\r\n",
    "          \"NEG\": \"linear-gradient(90deg, #ffc93c, #ffc93c)\"\r\n",
    "          }\r\n",
    " \r\n",
    "\r\n",
    "options = {\"compact\": False, \r\n",
    "           \"bg\": \"#09a3d5\",\r\n",
    "           \"color\": \"white\",\r\n",
    "           \"font\": \"Source Sans Pro\"}\r\n",
    "\r\n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options=options)\r\n",
    "\r\n",
    "print(example.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ''' \r\n",
    "    Add matcher from dictionaries \r\n",
    "\r\n",
    "    To create the patterns, each phrase has to be processed with the nlp object. If you have a trained pipeline loaded, doing this in a loop or list comprehension can easily become inefficient and slow. If you only need the tokenization and lexical attributes, you can run nlp.make_doc instead, which will only run the tokenizer. For an additional speed boost, you can also use the nlp.tokenizer.pipe method, which will process the texts as a stream.\r\n",
    "    from spacy.matcher import PhraseMatcher\r\n",
    "    '''\r\n",
    "\r\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\r\n",
    "# Only run nlp.make_doc to speed things up\r\n",
    "\r\n",
    "icd_patterns = [nlp.make_doc(text) for text in ICD]\r\n",
    "neg_patterns = [nlp.make_doc(text) for text in NEG]\r\n",
    "\r\n",
    "matcher.add(\"SYM\", icd_patterns)\r\n",
    "matcher.add(\"NEG\", neg_patterns)\r\n",
    "\r\n",
    "# This is not what we want to do, as this is not integrated as a pipe in the pipeline and is therfore harder to analyse\r\n",
    "matches = matcher(doc)\r\n",
    "entities = []\r\n",
    "for match_id, start, end in matches:\r\n",
    "    span = doc[start:end]\r\n",
    "    class_id = nlp.vocab.strings[match_id]\r\n",
    "    print(span.text, span.start_char, span.end_char, class_id)\r\n",
    "    ent = {\"start\": span.start_char, \"end\": span.end_char, \"label\": class_id}\r\n",
    "    entities.append(ent)\r\n",
    "print(entities)\r\n",
    "\r\n",
    "'''\r\n",
    " Changed in v3.0\r\n",
    "As of spaCy v3.0, PhraseMatcher.add takes a list of patterns as the second argument (instead of a variable number of arguments). The on_match callback becomes an optional keyword argument.\r\n",
    "\r\n",
    "```\r\n",
    "patterns = [nlp(\"health care reform\"), nlp(\"healthcare reform\")]\r\n",
    "- matcher.add(\"HEALTH\", on_match, *patterns)\r\n",
    "+ matcher.add(\"HEALTH\", patterns, on_match=on_match) \r\n",
    "``` \r\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY9e2WazlW3S"
   },
   "source": [
    "# ANALYSIS eXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu8s42_U4aYt"
   },
   "outputs": [],
   "source": [
    "# https://support.prodi.gy/t/evaluation-of-rule-based-matching/1431\r\n",
    "true_positives = guesses.intersection(truth)\r\n",
    "false_positives = guesses - truth\r\n",
    "false_negatives = truth - guesses\r\n",
    "\r\n",
    "precision = len(true_positives) / len(guesses)\r\n",
    "recall = len(true_positives) / len(truth)\r\n",
    "fscore = 2 * ((p * r) / (p + r + 1e-100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9vEGD235jXy"
   },
   "source": [
    "When you make your sets, make sure that you’re representing the spans by the start and end offsets with the label, instead of just the text. It’s not so relevant in your case, but it covers you if you do have inputs with multiple annotations that have the same text content. A tuple (start, end, label) will be hashable, so you can store it in a set.\n",
    "\n",
    "If you’re making the set over a whole dataset, you’ll also want to add in the input hash, to make sure you’re referring to the right examples. All up, it should be as easy as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "xSYilswIld7Q",
    "outputId": "dfdae177-cf21-48fd-aaa8-4f19e91c62f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "ignored",
       "name": "NameError",
       "stack": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<ipython-input-46-9dc46b53c065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# DB = connect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_annotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mguesses\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatcher_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRFScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mNameError\u001b[0m: name 'DB' is not defined"
      }
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "def get_annotations(dataset):\r\n",
    "    annotations = set()\r\n",
    "    for eg in dataset:\r\n",
    "        for span in eg[\"spans\"]:\r\n",
    "            annotations.add((span[\"start\"], span[\"end\"], span[\"label\"]))\r\n",
    "    return annotations\r\n",
    "\r\n",
    "# DB = connect()\r\n",
    "truth = get_annotations(DB.get_dataset(gold_annotations))\r\n",
    "guesses =  get_annotations(matcher_output)\r\n",
    "scores = spacy.scorer.PRFScore()\r\n",
    "scores.score_set(guesses, truth)\r\n",
    "print(scores.precision, scores.recall, scores.fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiOs3wKNjZkg"
   },
   "outputs": [],
   "source": [
    "# load an example dataset\n",
    "from vega_datasets import data\n",
    "cars = data.cars()\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "points = alt.Chart(cars).mark_point().encode(\n",
    "  x='Year:T',\n",
    "  y='Miles_per_Gallon',\n",
    "  color='Origin'\n",
    ").properties(\n",
    "  width=800\n",
    ")\n",
    "\n",
    "lines = alt.Chart(cars).mark_line().encode(\n",
    "  x='Year:T',\n",
    "  y='mean(Miles_per_Gallon)',\n",
    "  color='Origin'\n",
    ").properties(\n",
    "  width=800\n",
    ").interactive(bind_y=False)\n",
    "              \n",
    "points + lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIQWEvHd3J0S",
    "outputId": "b5069bd9-4f62-4539-b261-9ca5b9734c83"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "Create a new set:\n",
       "set()\n",
       "<class 'set'>\n",
       "\n",
       "Create a non empty set:\n",
       "{0, 1, 2, 3, 4}\n",
       "<class 'set'>\n",
       "\n",
       "Using a literal:\n",
       "<class 'set'>\n",
       "{1, 2, 3, 'foo', 'bar'}\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "print(\"Create a new set:\")\r\n",
    "x = set()\r\n",
    "print(x)\r\n",
    "print(type(x))\r\n",
    "print(\"\\nCreate a non empty set:\")\r\n",
    "n = set([0, 1, 2, 3, 4])\r\n",
    "print(n)\r\n",
    "print(type(n))\r\n",
    "print(\"\\nUsing a literal:\")\r\n",
    "a = {1,2,3,'foo','bar'}\r\n",
    "print(type(a))\r\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNpLIJ6i3Rxd"
   },
   "source": [
    "A set is an unordered collection of items. Every set element is unique (no duplicates) and must be immutable (cannot be changed).\n",
    "\n",
    "However, a set itself is mutable. We can add or remove items from it.\n",
    "\n",
    "Sets can also be used to perform mathematical set operations like union, intersection, symmetric difference, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "EYq_5KJM6Ohx",
    "outputId": "89ba0924-99e5-4ea5-cd24-547ea8b3290d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "ignored",
       "name": "NameError",
       "stack": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<ipython-input-45-1433ed23d696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training data for a part-of-speech tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"like\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgold_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"NOUN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VERB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NOUN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mNameError\u001b[0m: name 'Doc' is not defined"
      }
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "# Training data for a part-of-speech tagger\r\n",
    "doc = Doc(vocab, words=[\"I\", \"like\", \"stuff\"])\r\n",
    "gold_dict = {\"tags\": [\"NOUN\", \"VERB\", \"NOUN\"]}\r\n",
    "example = Example.from_dict(doc, gold_dict)\r\n",
    "\r\n",
    "# Training data for an entity recognizer (option 1)\r\n",
    "doc = nlp(\"Laura flew to Silicon Valley.\")\r\n",
    "gold_dict = {\"entities\": [\"U-PERS\", \"O\", \"O\", \"B-LOC\", \"L-LOC\"]}\r\n",
    "example = Example.from_dict(doc, gold_dict)\r\n",
    "\r\n",
    "# Training data for an entity recognizer (option 2)\r\n",
    "doc = nlp(\"Laura flew to Silicon Valley.\")\r\n",
    "gold_dict = {\"entities\": [(0, 5, \"PERSON\"), (14, 28, \"LOC\")]}\r\n",
    "example = Example.from_dict(doc, gold_dict)\r\n",
    "\r\n",
    "# Training data for text categorization\r\n",
    "doc = nlp(\"I'm pretty happy about that!\")\r\n",
    "gold_dict = {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}}\r\n",
    "example = Example.from_dict(doc, gold_dict)\r\n",
    "\r\n",
    "# Training data for an Entity Linking component (also requires entities & sentences)\r\n",
    "doc = nlp(\"Russ Cochran his reprints include EC Comics.\")\r\n",
    "gold_dict = {\"entities\": [(0, 12, \"PERSON\")],\r\n",
    "             \"links\": {(0, 12): {\"Q7381115\": 1.0, \"Q2146908\": 0.0}},\r\n",
    "             \"sent_starts\": [1, -1, -1, -1, -1, -1, -1, -1]}\r\n",
    "example = Example.from_dict(doc, gold_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYw6YjCZaV8M",
    "outputId": "f8379e16-63ea-44b5-c04a-933c3dbf9ba9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "/content/clinical_NLP_SE\n",
       "remote: Enumerating objects: 30, done.\u001b[K\n",
       "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
       "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
       "remote: Total 26 (delta 3), reused 26 (delta 3), pack-reused 0\u001b[K\n",
       "Unpacking objects: 100% (26/26), done.\n",
       "From https://github.com/callebalik/clinical_NLP_SE\n",
       "   212afca..76e8d07  master     -> origin/master\n",
       "Updating 212afca..76e8d07\n",
       "Fast-forward\n",
       " data/raw/corpus/conll2003/chart1.txt/admin.conll | 321 \u001b[32m++++++++++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/chart2.txt/admin.conll | 366 \u001b[32m++++++++++++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/chart3.txt/admin.conll | 280 \u001b[32m++++++++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/chart4.txt/admin.conll | 451 \u001b[32m+++++++++++++++++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/chart5.txt/admin.conll | 214 \u001b[32m+++++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/pat1.txt/admin.conll   | 220 \u001b[32m+++++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/pat2.txt/admin.conll   | 173 \u001b[32m+++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/pat3.txt/admin.conll   | 173 \u001b[32m+++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/pat4.txt/admin.conll   | 160 \u001b[32m++++++++\u001b[m\n",
       " data/raw/corpus/conll2003/pat5.txt/admin.conll   | 219 \u001b[32m+++++++++++\u001b[m\n",
       " 10 files changed, 2577 insertions(+)\n",
       " create mode 100644 data/raw/corpus/conll2003/chart1.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/chart2.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/chart3.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/chart4.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/chart5.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/pat1.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/pat2.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/pat3.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/pat4.txt/admin.conll\n",
       " create mode 100644 data/raw/corpus/conll2003/pat5.txt/admin.conll\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "%cd clinical_NLP_SE/\r\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWZW5_2fXVH4",
    "outputId": "1b05bb73-3999-440f-8468-ceb1ee4864cc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "2021-05-31 07:20:37.358995: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
       "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
       "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
       "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
       "into documents with `-n 10`.\u001b[0m\n",
       "\u001b[38;5;2m✔ Generated output file (34 documents):\n",
       "/content/clinical_NLP_SE/data/interim/admin.spacy\u001b[0m\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "!python -m spacy convert --converter ner /content/clinical_NLP_SE/data/raw/corpus/conll2003/chart1.txt/admin.conll /content/clinical_NLP_SE/data/interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zxw3bPZvb-Uh",
    "outputId": "4da4fb60-b529-4331-ceed-4e2ea1c0247b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "ignored",
       "name": "SyntaxError",
       "stack": "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-da84224cbfdb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python -m spacy debug-data de /content/clinical_NLP_SE/data/interim/admin.spacy -p ner -b de_core_news_md\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
      }
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "python -m spacy debug-data de /content/clinical_NLP_SE/data/interim/admin.spacy -p ner -b de_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdRZ9_6Onzbq",
    "outputId": "45317e06-3845-4dac-e421-101583291087"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (4.4)\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqk1fw0Yn5U5",
    "outputId": "d8d9d238-60ec-4c2c-c175-412af4e7cc9d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "TokenList<Kontaktorsak, :, Dyspné, Tidigare, sjukdomar, :, Insulin, och, tablettbehandlad, DM2, .>\n",
       "TokenList<Tablettbehandlad, hypertoni, .>\n",
       "TokenList<Höga, blodfetter, .>\n",
       "TokenList<Socialt, :, Ensamboende, ,, hemtjänst, *, 1, .>\n",
       "TokenList<Aktuellt, :, Insjuknat, för, tre, dagar, sedan, med, andnöd, vid, gång, .>\n",
       "TokenList<Tillkomst, av, hosta, under, gårdagen, .>\n",
       "TokenList<Ej, noterat, feber, ,, men, ej, eller, mätt, tempen, .>\n",
       "TokenList<Idag, nedsatt, AT, ,, svårt, att, gå, .>\n",
       "TokenList<Tagit, sina, mediciner, som, vanligt, .>\n",
       "TokenList<Sökt, VC, som, hänvisat, hit, pga, destruering, .>\n",
       "TokenList<Inte, haft, några, utslag, .>\n",
       "TokenList<Ej, kontakt, med, någon, annan, som, varit, sjuk, .>\n",
       "TokenList<Ej, varit, ute, och, rest, .>\n",
       "TokenList<Ingen, smärta, vid, miktion, ,, buksmärta, eller, ÖNH, symptom, .>\n",
       "TokenList<Idag, sämre, ,, kan, ej, mobilisera, sig, i, hemmet, .>\n",
       "TokenList<Inkommer, på, remiss, .>\n",
       "TokenList<Inga, kräkningar, eller, GI, symptom, .>\n",
       "TokenList<Status, :, AT, :, Gott, ,, opåverkad, i, vila, ,, ingen, samtalsdyspné, .>\n",
       "TokenList<Cor, :, Normofrekvent, regelbunden, rytm, ,, inga, hörbara, bi, eller, blåsljud, .>\n",
       "TokenList<Pulm, :, I, stort, vesikulära, antingsljud, bilat, ,, dämpat, basalt, höger, .>\n",
       "TokenList<Inget, förlängt, expirium, .>\n",
       "TokenList<Buk, :, Överallt, muk, och, oöm, .>\n",
       "TokenList<Ingen, lokal, peritonit, .>\n",
       "TokenList<Lokalstatus, :, Underben, smala, och, gracila, ,, inga, pittingödem, .>\n",
       "TokenList<BT, :, 117, /, 80, Sat, :, 92%, på, 2l, O2, AF, :, 28, Temp, :, 38.8, Lab, :, Blodgas, utan, respiratorisk, acidos, ,, Ph, 7,4, ,, CRP, 118, ,, diskret, förhöjt, BE, .>\n",
       "TokenList<Normalt, laktat, Tobak, :, Ej, känt, EKG, :, Sinusrym, med, kammarfrekvens, 98, ,, inga, urakuta, ischemiska, tecken, .>\n",
       "TokenList<Bedömning, :, I, första, hand, misstanke, om, penumoni, eller, COVID, -, 19, ,, inlägges, infektionsavdelning, .>\n",
       "TokenList<Provtages, SARS-CoV2, insättes, cefotaxim, .>\n",
       "TokenList<Genomodlas, .>\n",
       "TokenList<Nya, prover, imorgon, ,, CRP, ,, LPK, .>\n",
       "TokenList<Röntgen, puls, från, avdelningen, .>\n",
       "TokenList<Syrgaskrävande, ,, 2l, .>\n",
       "TokenList<Ej, septisk, .>\n",
       "TokenList<Diagnos, :, Pneumoni, ,, ospecificerad>\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    " from conllu import parse\r\n",
    " from io import open\r\n",
    "from conllu import parse_incr\r\n",
    "\r\n",
    "data_file = open(\"/content/clinical_NLP_SE/data/raw/corpus/mockup-patient-records/chart1.txt/admin.conllu\", \"r\", encoding=\"utf-8\")\r\n",
    "for tokenlist in parse_incr(data_file):\r\n",
    "    print(tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4qauXbJUeWio",
    "outputId": "d0e33c83-0059-40a0-b9ec-8d6fdebc8057"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "ignored",
       "name": "AttributeError",
       "stack": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n\u001b[0;32m<ipython-input-44-3bd58213ebb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/clinical_NLP_SE/data/interim/admin.spacy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdoc_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocBin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdoc_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data.spacy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data.spacy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, attrs, store_user_data, docs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_user_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_user_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/tokens/_serialize.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocbin\u001b[0m\u001b[0;31m#add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_array'"
      }
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Corpus\n",
    "\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk(\"./data.spacy\")\n",
    "reader = Corpus(\"./data.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "3HB-VzExa1xT",
    "outputId": "52f3c7f3-d696-465a-82ec-6484e3e08df6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "ignored",
       "name": "SyntaxError",
       "stack": "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-1ffc7d0e772e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    doc = nlp(/content/clinical_NLP_SE/data/interim/admin.spacy)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
      }
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "\n",
    "doc = nlp(/content/clinical_NLP_SE/data/interim/admin.spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gU_ZEVzLmVnk",
    "outputId": "0a3655c5-fedc-48a2-8699-c9b503c14fb1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "2021-05-28 10:51:12.555487: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
       "Usage: python -m spacy debug-data \n",
       "           [OPTIONS] CONFIG_PATH\n",
       "\n",
       "  Analyze, debug and validate your training and\n",
       "  development data. Outputs useful stats, and\n",
       "  can help you find problems like invalid entity\n",
       "  annotations, cyclic dependencies, low data\n",
       "  labels and more.\n",
       "\n",
       "  DOCS: https://spacy.io/api/cli#debug-data\n",
       "\n",
       "Arguments:\n",
       "  CONFIG_PATH  Path to config file  [required]\n",
       "\n",
       "Options:\n",
       "  -c, --code-path, --code PATH  Path to Python\n",
       "                                file with\n",
       "                                additional code\n",
       "                                (registered\n",
       "                                functions) to be\n",
       "                                imported\n",
       "\n",
       "  -IW, --ignore-warnings        Ignore warnings,\n",
       "                                only show stats\n",
       "                                and errors\n",
       "                                [default: False]\n",
       "\n",
       "  -V, --verbose                 Print additional\n",
       "                                information and\n",
       "                                explanations\n",
       "                                [default: False]\n",
       "\n",
       "  -NF, --no-format              Don't pretty-print\n",
       "                                the results\n",
       "                                [default: False]\n",
       "\n",
       "  --help                        Show this message\n",
       "                                and exit.\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "!python -m spacy debug-data --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "einSe99xbW1I",
    "outputId": "c9bf9ebc-5948-4828-a3a0-55dcdf86f352"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>space</th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>normalization</th>\n",
       "      <th>POS</th>\n",
       "      <th>explain</th>\n",
       "      <th>stopword</th>\n",
       "      <th>dep</th>\n",
       "      <th>NE</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>data_path</td>\n",
       "      <td>data_path</td>\n",
       "      <td>data_path</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>False</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>SYM</td>\n",
       "      <td>symbol</td>\n",
       "      <td>False</td>\n",
       "      <td>cc</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>interim</td>\n",
       "      <td>interim</td>\n",
       "      <td>interim</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>False</td>\n",
       "      <td>conj</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>SYM</td>\n",
       "      <td>symbol</td>\n",
       "      <td>False</td>\n",
       "      <td>cc</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>admin.spacy</td>\n",
       "      <td>admin.spacy</td>\n",
       "      <td>admin.spacy</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>False</td>\n",
       "      <td>conj</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   space         text        lemma normalization  ... stopword   dep  NE sentiment\n",
       "0  False    data_path    data_path     data_path  ...    False  ROOT           0.0\n",
       "1  False            /            /             /  ...    False    cc           0.0\n",
       "2  False      interim      interim       interim  ...    False  conj           0.0\n",
       "3  False            /            /             /  ...    False    cc           0.0\n",
       "4  False  admin.spacy  admin.spacy   admin.spacy  ...    False  conj           0.0\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "cols = (\"space\", \"text\", \"lemma\", \"normalization\", \"POS\", \"explain\", \"stopword\", \"dep\",\"NE\", \"sentiment\")\r\n",
    "rows = []\r\n",
    "\r\n",
    "for t in doc:\r\n",
    "      if not t.is_space:\r\n",
    "        row = [t.is_space, t.text, t.lemma_, t.norm_, t.pos_, spacy.explain(t.pos_), t.is_stop, t.dep_, t.ent_type_, t.sentiment]\r\n",
    "        rows.append(row)\r\n",
    "\r\n",
    "df = pd.DataFrame(rows, columns=cols)\r\n",
    "    \r\n",
    "df\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "8PF3IJOKqqVm",
    "outputId": "ba32fc36-7f27-4802-9873-23211fa45181"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "ignored",
       "name": "SyntaxError",
       "stack": "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-a99afaec213b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python -m spacy convert /path/to/input/doc.connlu /path/to/output/doc.jsonl -c conllu.\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
      }
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "!python -m spacy convert /content/clinical_NLP_SE/data/raw/corpus/mockup-patient-records/chart1.txt/admin.conllu /path/to/output/doc.jsonl -c conllu."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8826b53f20e9cd7110a7f40447e35f8ee8fd51c1cc996b01684cb7063458cfdb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}